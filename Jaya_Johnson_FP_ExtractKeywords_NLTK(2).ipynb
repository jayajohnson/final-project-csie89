{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Jaya-Johnson-FP-ExtractKeywords-NLTK(2).ipynb",
      "version": "0.3.2",
      "provenance": [],
      "private_outputs": true,
      "collapsed_sections": [],
      "toc_visible": true,
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n3ZAf5lVAgWG",
        "colab_type": "text"
      },
      "source": [
        "## Data - Prep\n",
        "Follow instructions below to prepare files for data extraction.(Alternatively you can modify the code to upload the zip files and change the references)\n",
        "\n",
        "1) Create the following folder in your google drive final-project/raw \n",
        "\n",
        "2) Upload the 4 zipped files included in the submission to the folder above.\n",
        "\n",
        "3) Run the code in cell 1 to mound the drive."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fgw0vDKyCO_k",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WDt8Ou3wGkuS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import zipfile\n",
        "\n",
        "with zipfile.ZipFile('/content/drive/My Drive/final-project/raw/2018q3_notes.zip', 'r') as zip_ref:\n",
        "    zip_ref.extractall('/content/drive/My Drive/final-project/raw/extracted/2018q3')\n",
        "with zipfile.ZipFile('/content/drive/My Drive/final-project/raw/2018q4_notes.zip', 'r') as zip_ref:\n",
        "    zip_ref.extractall('/content/drive/My Drive/final-project/raw/extracted/2018q4')\n",
        "with zipfile.ZipFile('/content/drive/My Drive/final-project/raw/2019q1_notes.zip', 'r') as zip_ref:\n",
        "    zip_ref.extractall('/content/drive/My Drive/final-project/raw/extracted/2019q1')\n",
        "with zipfile.ZipFile('/content/drive/My Drive/final-project/raw/2019q2_notes.zip', 'r') as zip_ref:\n",
        "    zip_ref.extractall('/content/drive/My Drive/final-project/raw/extracted/2019q2')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ula5XGO9Qxbe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "with open(\"/content/drive/My Drive/final-project/raw/extracted/2018q3/txt.tsv\") as myfile:\n",
        "    head = [next(myfile) for x in range(10)]\n",
        "print(head)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DAJ0OrvDr_bf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "#folders = ['2018q3/','2018q4/','2019q1/','2019q2/']\n",
        "folders = ['2018q3/', '2018q4/']\n",
        "extract_folder = '/content/drive/My Drive/final-project/raw/extracted/'\n",
        "context_file = 'txt.tsv'\n",
        "\n",
        "filenames = []\n",
        "for folder in folders:\n",
        "  filename = extract_folder + folder + context_file\n",
        "  filenames.append(filename)\n",
        "print(filenames)\n",
        "  \n",
        "dfs = pd.concat([pd.read_csv(f, sep='\\t') for f in filenames], ignore_index = True)\n",
        "    \n",
        "print(dfs.columns.values)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OuGMaSL-60Ha",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "##print(dfs.loc[: , \"value\"])\n",
        "#Fetch wordcount for each abstract\n",
        "dfs['word_count'] = dfs['value'].apply(lambda x: len(str(x).split(\" \")))\n",
        "dfs[['value','word_count']].head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bOp0_8p38DCV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "##Descriptive statistics of word counts\n",
        "dfs.word_count.describe()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tkkNHzBE8RBx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Identify common words\n",
        "import pandas\n",
        "#Identify common words\n",
        "freq = pandas.Series(''.join(map(str,dfs['value'])).split()).value_counts()[:20]\n",
        "freq"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sP_OCZUsMKmo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Identify uncommon words\n",
        "freq1 =  pandas.Series(''.join(map(str,dfs['value'])).split()).value_counts()[:-20]\n",
        "freq1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bbdbx90mNXFg",
        "colab_type": "text"
      },
      "source": [
        "# Pre Processing the text.\n",
        "Now that we have the basic stats lets do some pre processing to remove noise and normalize the data.Data components that are redundant to the core text analytics can be considered as noise."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FCqBURD6PvKE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import nltk\n",
        "import re\n",
        "nltk.download('wordnet')\n",
        "from nltk.stem.porter import PorterStemmer\n",
        "from nltk.stem.wordnet import WordNetLemmatizer\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem.porter import PorterStemmer\n",
        "from nltk.tokenize import RegexpTokenizer \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "stQj8_RIV-22",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "##Creating a list of stop words and adding custom stopwords\n",
        "stop_words = set(stopwords.words(\"english\"))\n",
        "##Creating a list of custom stopwords\n",
        "new_words = ['Jan', 'Janurary', 'Feb', 'February', 'March', 'April', 'May', 'Jun', 'June', 'July',\n",
        "            'Aug', 'August', 'Sept', 'September', 'Oct', 'October', 'Nov', 'November', 'Dec', 'December', \n",
        "             'Month', 'Ended', 'Ending', 'Three', 'Period']\n",
        "stop_words = stop_words.union(new_words)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "814Ag-fWWRWL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "corpus = []\n",
        "for i in range(0, 5000):\n",
        "    #Remove punctuations\n",
        "    text = re.sub('[^a-zA-Z]', ' ', str(dfs['value'][i]))\n",
        "    \n",
        "    #Convert to lowercase\n",
        "    text = text.lower()\n",
        "    \n",
        "    #remove tags\n",
        "    text=re.sub(\"&lt;/?.*?&gt;\",\" &lt;&gt; \",text)\n",
        "    \n",
        "    # remove special characters and digits\n",
        "    text=re.sub(\"(\\\\d|\\\\W)+\",\" \",text)\n",
        "    \n",
        "    ##Convert to list from string\n",
        "    text = text.split()\n",
        "    \n",
        "    ##Stemming\n",
        "    ps=PorterStemmer()\n",
        "    #Lemmatisation\n",
        "    lem = WordNetLemmatizer()\n",
        "    text = [lem.lemmatize(word) for word in text if not word in  \n",
        "            stop_words]\n",
        "    text = [ word for word in text if len(word) > 3 ]\n",
        "    if len(text) > 0:\n",
        "      text = \" \".join(text)\n",
        "      corpus.append(text)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JfqmSTn9YpyZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#View corpus item\n",
        "corpus[:100]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oEeec1c0d0Ge",
        "colab_type": "text"
      },
      "source": [
        "#Data Exploration\n",
        "We will now visualize the text corpus that we created after pre-processing to get insights on the most frequently used words."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7ai_L15BeB8l",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from os import path\n",
        "from PIL import Image\n",
        "from wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\n",
        "import matplotlib.pyplot as plt\n",
        "% matplotlib inline\n",
        "wordcloud = WordCloud(\n",
        "                          background_color='white',\n",
        "                          stopwords=stop_words,\n",
        "                          max_words=100,\n",
        "                          max_font_size=50, \n",
        "                          random_state=42\n",
        "                         ).generate(str(corpus))\n",
        "print(wordcloud)\n",
        "fig = plt.figure(1)\n",
        "plt.imshow(wordcloud)\n",
        "plt.axis('off')\n",
        "plt.show()\n",
        "fig.savefig(\"word1.png\", dpi=900)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rc8kLgKPfizd",
        "colab_type": "text"
      },
      "source": [
        "# Text preparation\n",
        "\n",
        "Text in the corpus needs to be converted to a format that can be interpreted by the machine learning algorithms. There are 2 parts of this conversion — Tokenisation and Vectorisation.\n",
        "\n",
        "For text preparation we use the bag of words model which ignores the sequence of the words and only considers word frequencies.\n",
        "\n",
        "# Creating a vector of word counts\n",
        "\n",
        "As the first step of conversion, we will use the CountVectoriser to tokenise the text and build a vocabulary of known words. We first create a variable “cv” of the CountVectoriser class, and then evoke the fit_transform function to learn and build the vocabulary."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NUYYcFIhgCN5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "import re\n",
        "cv=CountVectorizer(max_df=0.8,stop_words=stop_words, max_features=10000, ngram_range=(1,3))\n",
        "X=cv.fit_transform(corpus)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nJV4vEs8if4A",
        "colab_type": "text"
      },
      "source": [
        "#Parameters passed to the Vectorizer function\n",
        "cv=CountVectorizer(max_df=0.8,stop_words=stop_words, max_features=10000, ngram_range=(1,3))\n",
        "\n",
        "max_df — When building the vocabulary ignore terms that have a document frequency strictly higher than the given threshold (corpus-specific stop words). This is to ensure that we only have words relevant to the context and not commonly used words.\n",
        "\n",
        "max_features — determines the number of columns in the matrix.\n",
        "\n",
        "n-gram range — we would want to look at a list of single words, two words (bi-grams) and three words (tri-gram) combinations."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pUnFFX_bigbQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "list(cv.vocabulary_.keys())[:50]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QFBBcgybkMSS",
        "colab_type": "text"
      },
      "source": [
        "#Visualize top N uni-grams, bi-grams & tri-grams\n",
        "\n",
        "We can use the CountVectoriser to visualise the top 20 unigrams, bi-grams and tri-grams."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1_81LUW_kUT5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Most frequently occuring words\n",
        "def get_top_n_words(corpus, n=None):\n",
        "    vec = CountVectorizer().fit(corpus)\n",
        "    bag_of_words = vec.transform(corpus)\n",
        "    sum_words = bag_of_words.sum(axis=0) \n",
        "    words_freq = [(word, sum_words[0, idx]) for word, idx in      \n",
        "                   vec.vocabulary_.items()]\n",
        "    words_freq =sorted(words_freq, key = lambda x: x[1], \n",
        "                       reverse=True)\n",
        "    return words_freq[:n]\n",
        "#Convert most freq words to dataframe for plotting bar plot\n",
        "top_words = get_top_n_words(corpus, n=20)\n",
        "top_df = pandas.DataFrame(top_words)\n",
        "top_df.columns=[\"Word\", \"Freq\"]\n",
        "#Barplot of most freq words\n",
        "import seaborn as sns\n",
        "sns.set(rc={'figure.figsize':(13,8)})\n",
        "g = sns.barplot(x=\"Word\", y=\"Freq\", data=top_df)\n",
        "g.set_xticklabels(g.get_xticklabels(), rotation=30)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U6Uu2ZmtlC9x",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Most frequently occuring Bi-grams\n",
        "def get_top_n2_words(corpus, n=None):\n",
        "    vec1 = CountVectorizer(ngram_range=(2,2),  \n",
        "            max_features=2000).fit(corpus)\n",
        "    bag_of_words = vec1.transform(corpus)\n",
        "    sum_words = bag_of_words.sum(axis=0) \n",
        "    words_freq = [(word, sum_words[0, idx]) for word, idx in     \n",
        "                  vec1.vocabulary_.items()]\n",
        "    words_freq =sorted(words_freq, key = lambda x: x[1], \n",
        "                reverse=True)\n",
        "    return words_freq[:n]\n",
        "top2_words = get_top_n2_words(corpus, n=20)\n",
        "top2_df = pandas.DataFrame(top2_words)\n",
        "top2_df.columns=[\"Bi-gram\", \"Freq\"]\n",
        "print(top2_df)\n",
        "#Barplot of most freq Bi-grams\n",
        "import seaborn as sns\n",
        "sns.set(rc={'figure.figsize':(13,8)})\n",
        "h=sns.barplot(x=\"Bi-gram\", y=\"Freq\", data=top2_df)\n",
        "h.set_xticklabels(h.get_xticklabels(), rotation=45)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VlYdSUVRlLJW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Most frequently occuring Tri-grams\n",
        "def get_top_n3_words(corpus, n=None):\n",
        "    vec1 = CountVectorizer(ngram_range=(3,3), \n",
        "           max_features=2000).fit(corpus)\n",
        "    bag_of_words = vec1.transform(corpus)\n",
        "    sum_words = bag_of_words.sum(axis=0) \n",
        "    words_freq = [(word, sum_words[0, idx]) for word, idx in     \n",
        "                  vec1.vocabulary_.items()]\n",
        "    words_freq =sorted(words_freq, key = lambda x: x[1], \n",
        "                reverse=True)\n",
        "    return words_freq[:n]\n",
        "top3_words = get_top_n3_words(corpus, n=20)\n",
        "top3_df = pandas.DataFrame(top3_words)\n",
        "top3_df.columns=[\"Tri-gram\", \"Freq\"]\n",
        "print(top3_df)\n",
        "#Barplot of most freq Tri-grams\n",
        "import seaborn as sns\n",
        "sns.set(rc={'figure.figsize':(13,8)})\n",
        "j=sns.barplot(x=\"Tri-gram\", y=\"Freq\", data=top3_df)\n",
        "j.set_xticklabels(j.get_xticklabels(), rotation=45)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t0guhd-xmf_V",
        "colab_type": "text"
      },
      "source": [
        "#Converting to a matrix of integers\n",
        "\n",
        "The next step of refining the word counts is using the TF-IDF vectoriser. The deficiency of a mere word count obtained from the countVectoriser is that, large counts of certain common words may dilute the impact of more context specific words in the corpus. This is overcome by the TF-IDF vectoriser which penalizes words that appear several times across the document. TF-IDF are word frequency scores that highlight words that are more important to the context rather than those that appear frequently across documents.\n",
        "\n",
        "TF-IDF consists of 2 components:\n",
        "\n",
        "TF — term frequency\n",
        "\n",
        "IDF — Inverse document frequency"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ARkPCfv-mxed",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.feature_extraction.text import TfidfTransformer\n",
        " \n",
        "tfidf_transformer=TfidfTransformer(smooth_idf=True,use_idf=True)\n",
        "tfidf_transformer.fit(X)\n",
        "# get feature names\n",
        "feature_names=cv.get_feature_names()\n",
        " \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ytt1NVtcoEOw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Function for sorting tf_idf in descending order\n",
        "from scipy.sparse import coo_matrix\n",
        "def sort_coo(coo_matrix):\n",
        "    tuples = zip(coo_matrix.col, coo_matrix.data)\n",
        "    return sorted(tuples, key=lambda x: (x[1], x[0]), reverse=True)\n",
        " \n",
        "def extract_topn_from_vector(feature_names, sorted_items, topn=10):\n",
        "    \"\"\"get the feature names and tf-idf score of top n items\"\"\"\n",
        "    \n",
        "    #use only topn items from vector\n",
        "    sorted_items = sorted_items[:topn]\n",
        " \n",
        "    score_vals = []\n",
        "    feature_vals = []\n",
        "    \n",
        "    # word index and corresponding tf-idf score\n",
        "    for idx, score in sorted_items:\n",
        "        \n",
        "        #keep track of feature name and its corresponding score\n",
        "        score_vals.append(round(score, 3))\n",
        "        feature_vals.append(feature_names[idx])\n",
        " \n",
        "    #create a tuples of feature,score\n",
        "    #results = zip(feature_vals,score_vals)\n",
        "    results= {}\n",
        "    for idx in range(len(feature_vals)):\n",
        "        results[feature_vals[idx]]=score_vals[idx]\n",
        "    \n",
        "    return results\n",
        "\n",
        "# now print the results\n",
        "context_keywords_file = open(\"/content/drive/My Drive/final-project/keywords/keyword-context.txt\", \"w+\")\n",
        "\n",
        "# fetch document for which keywords needs to be extracted\n",
        "for doc in corpus:\n",
        "  #generate tf-idf for the given document\n",
        "  tf_idf_vector=tfidf_transformer.transform(cv.transform([doc]))  \n",
        "  #sort the tf-idf vectors by descending order of scores\n",
        "  sorted_items=sort_coo(tf_idf_vector.tocoo())\n",
        "  #extract only the top n; n here is 10\n",
        "  keywords=extract_topn_from_vector(feature_names,sorted_items,5)\n",
        "\n",
        "\n",
        "  print(\"\\nContext:\")\n",
        "  print(doc)\n",
        "  print(\"\\nKeywords:\")\n",
        "  for k in keywords:\n",
        "      # only use keywords that have more than 2 words\n",
        "      if len(k.split()) >= 2:\n",
        "        print(k, keywords[k])\n",
        "        context_keywords_file.write('{},{}\\n'.format(doc,k))\n",
        "        \n",
        "context_keywords_file.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9V-HH4MVrgB1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import files\n",
        "files.download('/content/drive/My Drive/final-project/keywords/keyword-context.txt')"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}